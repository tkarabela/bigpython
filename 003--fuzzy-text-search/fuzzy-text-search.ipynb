{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introductory-residence",
   "metadata": {},
   "source": [
    "# <center>How **Fuzzy Text Search** Works<br><small>Real-time search in 40 lines of Python</small></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Introduction**\n",
    "- Fuzzy autocomplete for 60,000 cities in the UK <span style=\"color:#888\">(using <code style=\"color:#888\">rapidfuzz</code>, 2 lines of code)</span>\n",
    "- `fuzzywuzzy`, `rapidfuzz` and `difflib` libraries in Python\n",
    "\n",
    "## **How does it work?**\n",
    "\n",
    "- When are two strings \"similar\"?\n",
    "- Hamming distance\n",
    "- Levenshtein distance, dynamic programming approach (Wagner-Fischer)\n",
    "- Vectorization, closing the gap between Python and C\n",
    "- Fuzzy autocomplete for 60,000 cities in the UK <span style=\"color:#888\">(from scratch, 40 lines of code)</span>\n",
    "\n",
    "\n",
    "<br class=\"vtab\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-bundle",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "divine-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz as fuzzywuzzy_fuzz, process as fuzzywuzzy_process\n",
    "from rapidfuzz import fuzz as rapidfuzz_fuzz, process as rapidfuzz_process\n",
    "import Levenshtein\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from functools import wraps\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def print_timing(f):\n",
    "    @wraps(f)\n",
    "    def inner(*args, **kwargs):\n",
    "        t0 = timer()\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        finally:\n",
    "            print(f\"\\n{f.__name__} finished in {1e3*(timer()-t0):.1f} ms\")\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-casting",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-newsletter",
   "metadata": {},
   "source": [
    "# <center>Introduction</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's say you want autocomplete for cities in Great Britain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sound-violin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86224fc7efbc4184826e410f6fa3ead4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='input_text'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#    You can download this dataset here:\n",
    "# ðŸ“¥ https://geoportal.statistics.gov.uk/datasets/a6c138d17ac54532b0ca8ee693922f10_0\n",
    "\n",
    "df = pd.read_csv(\"Index_of_Place_Names_in_Great_Britain_(July_2016).csv\")\n",
    "cities = df[\"place15nm\"].unique()\n",
    "\n",
    "@print_timing\n",
    "def interact_cities(input_text):\n",
    "    if not input_text: return\n",
    "    for city, score, _ in rapidfuzz_process.extract(input_text, cities, limit=10):\n",
    "        print(f\"{city:60}{score:3.2f}\")\n",
    "\n",
    "widgets.interact(interact_cities, input_text=\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-meaning",
   "metadata": {},
   "source": [
    "<br class=\"vtab\">\n",
    "\n",
    "**\"Fuzzy\" (approximate)** string matching looks for strings that are **similar** to a given string. Here are some great libraries you can use for this task in Python:\n",
    "\n",
    "- **`fuzzywuzzy`**, a fuzzy text matching library ported to many languages, including JS ([GitHub](https://github.com/seatgeek/fuzzywuzzy))\n",
    "- **`rapidfuzz`**, a performance-oriented version of `fuzzywuzzy` ([GitHub](https://github.com/maxbachmann/rapidfuzz))\n",
    "- **`difflib`** from Python standard library ([docs](https://docs.python.org/3/library/difflib.html))\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "All of these provide functions to compute \"similarity\" of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "communist-algeria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzywuzzy_fuzz.ratio(\"Tokyo and Osaka\", \"Tokio and Osaka\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-knife",
   "metadata": {},
   "source": [
    "And, based on that, to find best matching strings for given input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prostate-equality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('knight', 83), ('nigh', 80), ('knighthood', 75), ('knuth', 73), ('the', 72)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"knight\", \"knuth\", \"nigh\", \"ignite\", \"knighthood\", \"knead\", \"the\"]\n",
    "fuzzywuzzy_process.extract(\"knigth\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-database",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "You can choose from different **ranking** functions or even create your own. The British cities example uses `fuzz.WRatio`, a mix of different ranking approaches (see [`fuzz.py:224`](https://github.com/seatgeek/fuzzywuzzy/blob/9e3d2fe0d8c1b195696d5fbcda78c371dd4a6b8f/fuzzywuzzy/fuzz.py#L224)).\n",
    "\n",
    "- The search is made up from common **building blocks** to judge similarity.\n",
    "- In this tutorial, we'll implement some of them **from scratch using Python**\n",
    "- and **optimize them** so that we get something comparable to the first example:\n",
    "- fuzzy search in 60K strings in \"real time\" (<100ms).\n",
    "\n",
    "Note that `rapidfuzz` uses optimized C++ routines, so approaching its runtime performance will require some tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-repository",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br class=\"vtab\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-richardson",
   "metadata": {},
   "source": [
    "# <center>How does it work?</center>\n",
    "<br><br>\n",
    "\n",
    "### String similarity\n",
    "\n",
    "When two strings are kind of similar, we need to go beyond `True`/`False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "civilian-triumph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"mancesther\" == \"manchester\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pressing-queensland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzywuzzy_fuzz.ratio(\"mancesther\", \"manchester\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-reason",
   "metadata": {},
   "source": [
    "How do you measure **similarity** of two strings **a**, **b**?\n",
    "- Number of editing operations to get from **a** to **b**\n",
    "    - Replace (Hamming)\n",
    "    - Replace, insert, delete (Levenshtein)\n",
    "    - Replace, insert, delete, transpose (Levenshtein-Damerau)\n",
    "    - ...\n",
    "- Longest common substring\n",
    "- Common word count\n",
    "- Letter frequency\n",
    "- ...\n",
    "\n",
    "<br>\n",
    "\n",
    "Depending on the application (are we comparing single words? paragraphs? DNA sequences?), some similarity measures will be more appropriate than others. In this tutorial, we'll focus on **edit distance** measures.\n",
    "\n",
    "<br class=\"vtab\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-distributor",
   "metadata": {},
   "source": [
    "### Hamming distance\n",
    "\n",
    "**Hamming distance** is the number of letters one has to **replace** to get from string **a** to string **b**.\n",
    "\n",
    "- `d(\"cat\", \"hat\") = 1`, replace 1st letter (`c -> h`)\n",
    "- `d(\"cat\", \"lag\") = 2`, replace 1st letter (`c -> l`) and 3rd letter (`t -> g`)\n",
    "- `d(\"cat\", \"cats\")` is conventionally not defined (it cannot be achieved by replacing letters)\n",
    "\n",
    "<br>\n",
    "\n",
    "Hamming distance is used in study of error-correcting codes (encoding schemes that are resilient to some number of corrupt bits). Its major **limitation** in that it doesn't handle *misaligned* sequences well.\n",
    "\n",
    "<pre>\n",
    "d(\"Hamming <b>distance</b>\",\n",
    "  \"Hamming<b>distance</b> \") = 9\n",
    "          <span style=\"color:red\">xxxxxxxxx</span>\n",
    "</pre>\n",
    "\n",
    "<br class=\"vtab\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-alarm",
   "metadata": {},
   "source": [
    "Let's try implementing it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "regional-internet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e04d0cc0b304f86b9a2feade1ccaad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='input_text'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def hamming_distance(a: str, b: str):\n",
    "    if len(a) == len(b):\n",
    "        return sum(1 for a_i, b_i in zip(a, b) if a_i != b_i)\n",
    "    else:\n",
    "        return float(\"inf\")  # cannot get different length\n",
    "\n",
    "\n",
    "@print_timing\n",
    "def interact_cities_hamming(input_text):\n",
    "    if not input_text: return\n",
    "    for score, city in list(sorted((hamming_distance(input_text, s), s) for s in cities))[:10]:\n",
    "        print(f\"{city:60}{score:3.2f}\")\n",
    "\n",
    "widgets.interact(interact_cities_hamming, input_text=\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-example",
   "metadata": {},
   "source": [
    "<br class=\"vtab\">\n",
    "\n",
    "### Levenshtein distance\n",
    "\n",
    "**Hamming distance** is useful in specific contexts (like error-correcting codes), but for natural language, it's not very suitable. We'd like a more robust edit distance.\n",
    "\n",
    "**Levenshtein distance** is widely used for this reason. It has these operations:\n",
    "- Substitution (like Hamming)\n",
    "- Insertion\n",
    "- Deletion\n",
    "\n",
    "<br>The distance is defined as the *minimal* number of edits. There may be multiple minimal edits.\n",
    "\n",
    "- `d(\"cat\", \"cats\") = 1`, as in `cats -> cat`\n",
    "- `d(\"moon\", \"monsoon\") = 3`, as in `monsoon -> mnsoon -> msoon -> moon`\n",
    "- `d(\"knight\", \"knigth\") = 2`, as in `knigth -> knigh -> knight`\n",
    "\n",
    "Reading both strings from the left, *it's not obvious* which operation should be chosen at any given position. It's tempting to do nothing when characters match, but deleting or inserting characters instead may better align the remaining portion.\n",
    "\n",
    "<pre>\n",
    "d(\"Hamming <b>distance</b>\",\n",
    "  \"Hamming<b>distance</b>\") = 1\n",
    "          <span style=\"color:red\">insert \" \", not replace \"d\" -> \" \"!</span>\n",
    "</pre>\n",
    "\n",
    "<br>\n",
    "\n",
    "How to solve this conundrum?\n",
    "\n",
    "- We can simply try all possibilities using **recursion**...\n",
    "- ...which can be optimized using **dynamic programming** into the [Wagner-Fischer algorithm](https://en.wikipedia.org/wiki/Wagner%E2%80%93Fischer_algorithm).\n",
    "- An alternative approach is to use **finite automata**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-safety",
   "metadata": {},
   "source": [
    "<br class=\"vtab\">\n",
    "\n",
    "### Levenshtein distance with DP (Wagner-Fischer)\n",
    "\n",
    "For brevity, we'll only discuss one algorithm for Levenshtein distance &mdash; the [Wagner-Fischer algorithm](https://en.wikipedia.org/wiki/Wagner%E2%80%93Fischer_algorithm), which runs in $\\mathcal{O}(n^2)$ time and memory space for words of length $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "raised-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(a: str, b: str, verbose: bool = False) -> int:\n",
    "    m, n = len(a), len(b)\n",
    "    d = np.zeros((m+1, n+1), dtype=int)  # d[i,j] = levenshtein_distance(a[:i], b[:j])\n",
    "        \n",
    "    # when the other string is empty, distance is length of non-empty string\n",
    "    for i in range(m+1): d[i, 0] = i\n",
    "    for i in range(n+1): d[0, i] = i\n",
    "    \n",
    "    for j in range(1, n+1):\n",
    "        for i in range(1, m+1):\n",
    "            cost = 1 if a[i-1] != b[j-1] else 0\n",
    "            d[i, j] = min(d[i-1, j-1] + cost,   # substitute         â†˜\n",
    "                          d[i, j-1]   + 1,      # delete from B      â†’\n",
    "                          d[i-1, j]   + 1)      # insert into B      â†“\n",
    "    \n",
    "    if verbose: print(d)\n",
    "    return d[m, n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "armed-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4 5 6 7]\n",
      " [1 1 2 3 4 4 5 6]\n",
      " [2 2 2 3 4 5 4 5]\n",
      " [3 3 3 3 4 5 5 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = levenshtein_distance(\"cat\", \"wildcat\", verbose=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-graphic",
   "metadata": {},
   "source": [
    "<br class=\"vtab\">\n",
    "\n",
    "### Can we search in real time now?\n",
    "\n",
    "Compared to optimized C implementation, our `levenshtein_distance()` is **100-500x slower**.\n",
    "\n",
    "This is to be expected from this kind of code in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "military-surveillance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 ns Â± 7.32 ns per loop (mean Â± std. dev. of 7 runs, 10000000 loops each)\n",
      "142 ns Â± 3.09 ns per loop (mean Â± std. dev. of 7 runs, 10000000 loops each)\n",
      "307 ns Â± 3.2 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# optimized C implementation, https://pypi.org/project/python-Levenshtein\n",
    "%timeit Levenshtein.distance(\"cat\", \"rat\")\n",
    "%timeit Levenshtein.distance(\"knight\", \"knigth\")\n",
    "%timeit Levenshtein.distance(\"unimaginable\", \"imagination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "atmospheric-choice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.9 Âµs Â± 54.2 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\n",
      "44.6 Âµs Â± 376 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)\n",
      "167 Âµs Â± 6.38 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit levenshtein_distance(\"cat\", \"rat\")\n",
    "%timeit levenshtein_distance(\"knight\", \"knigth\")\n",
    "%timeit levenshtein_distance(\"unimaginable\", \"imagination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-wisdom",
   "metadata": {},
   "source": [
    "<br>\n",
    "  \n",
    "Let's try the British cities example with this implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "peaceful-implementation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e5541bcf374f35bd949b0d44890b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='input_text'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@print_timing\n",
    "def interact_cities_levenshtein(input_text):\n",
    "    n = len(input_text)\n",
    "    def rank(s):\n",
    "        m = len(s)\n",
    "        if abs(m - n) > 5:  # ðŸ”§ cheating a bit - we'll skip words which would have\n",
    "            return 0        #                     a large distance anyway\n",
    "        elif m == n:\n",
    "            dist = hamming_distance(input_text, s)  # ðŸ”§ not cheating, gives the same results\n",
    "        else:\n",
    "            dist = levenshtein_distance(input_text, s)\n",
    "        return 1.0 - dist/max(n, m)\n",
    "        \n",
    "    if not input_text: return\n",
    "    for score, city in list(sorted(((rank(s), s) for s in cities), reverse=True))[:10]:\n",
    "        print(f\"{city:60}{100*score:3.2f}\")\n",
    "\n",
    "widgets.interact(interact_cities_levenshtein, input_text=\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-banana",
   "metadata": {},
   "source": [
    "<br class=\"vtab\">\n",
    "\n",
    "### Vectorization\n",
    "\n",
    "Our Wagner-Fischer in Python is close, but not quite there for real-time querying. One possible thing to do here would be to optimize the function as it is, eg. using **Cython** or **Numba**.\n",
    "\n",
    "However, there is a different way &mdash; we can **compute many distances at once**, using the exact same logic. Indeed, for all strings of the same length, the algorithm does exactly the same steps, just the values in the table are different.\n",
    "\n",
    "Instead of one 2D table, we can have a \"3D table\", really just many 2D tables stacked together. Then `d[i,j]` will not be just one number, but a **vector** holding the results for all processed strings at once.\n",
    "\n",
    "This should be significantly faster, because the operations we need (sum, comparison, minimum) can be executed very efficiently for vectors using eg. NumPy. **The Python function itself will be as slow as ever, but we'll only need to call it once**, not thousands of times. Note that this doesn't change the asymptotic running time at all, it's just about the efficiency of Python vs. native code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wireless-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance_vec(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    m, n, k = len(a), len(b), b.shape[1]\n",
    "    d = np.zeros((m+1, n+1, k), dtype=np.uint16)  # d[i,j] = levenshtein_distance(a[:i], b[:j])\n",
    "\n",
    "    # when the other string is empty, distance is length of non-empty string\n",
    "    for i in range(m+1): d[i, 0] = i\n",
    "    for i in range(n+1): d[0, i] = i\n",
    "    \n",
    "    for j in range(1, n+1):\n",
    "        for i in range(1, m+1):\n",
    "            cost = a[i-1] != b[j-1]\n",
    "            d[i, j] = np.min([d[i-1, j-1] + cost,        # substitute         â†˜\n",
    "                              d[i, j-1]   + 1,           # delete from B      â†’\n",
    "                              d[i-1, j]   + 1], axis=0)  # insert into B      â†“\n",
    "\n",
    "    return d[m, n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "valued-skill",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e3bcdbaeeb4477a10252c3790c0b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='input_text'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert input strings to NumPy matrices\n",
    "len_to_cities = {}\n",
    "for w in cities: len_to_cities.setdefault(len(w), []).append(w)\n",
    "len_to_mat = {n: np.asarray([[ord(c) for c in w] for w in ws], dtype=np.uint16).T\n",
    "              for n, ws in len_to_cities.items()}\n",
    "\n",
    "@print_timing\n",
    "def interact_cities_levenshtein_vec(input_text):\n",
    "    a_vec = np.asarray([ord(c) for c in input_text])\n",
    "    n = len(input_text)\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    for m, b_mat in len_to_mat.items():\n",
    "        if abs(m - n) > 5: continue  # ðŸ”§ again, skipping strings with obvious large distance\n",
    "        dist = levenshtein_distance_vec(a_vec, b_mat)\n",
    "        scores = 1.0 - dist/max(n, m)\n",
    "        xs.extend(len_to_cities[m])\n",
    "        ys.extend(scores) \n",
    "    \n",
    "    if not input_text: return\n",
    "    for score, city in list(sorted(zip(ys, xs), reverse=True))[:10]:\n",
    "        print(f\"{city:60}{score:3.2f}\")\n",
    "\n",
    "widgets.interact(interact_cities_levenshtein_vec, input_text=\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-surge",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Finally, let's compare the perfomance with Levenshtein in C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "computational-static",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "levenshtein_distance\n",
      "5.47 s Â± 0 ns per loop (mean Â± std. dev. of 1 run, 1 loop each)\n",
      "\n",
      "levenshtein_distance_vec\n",
      "153 ms Â± 2.42 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "C implementation\n",
      "17.8 ms Â± 214 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "a = \"londen\"\n",
    "a_vec = np.asarray([ord(c) for c in a])\n",
    "\n",
    "print(\"levenshtein_distance\")\n",
    "%timeit -r 1 -n 1 [levenshtein_distance(a, b) for b in cities]\n",
    "print(\"\\nlevenshtein_distance_vec\")\n",
    "%timeit [levenshtein_distance_vec(a_vec, b_mat) for b_mat in len_to_mat.values()]\n",
    "print(\"\\nC implementation\")\n",
    "%timeit [Levenshtein.distance(a, b) for b in cities]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-grill",
   "metadata": {},
   "source": [
    "While the initial implementation is ~300x slower, the vectorized one is just ~10x slower."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Big Python",
   "language": "python",
   "name": "bigpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
